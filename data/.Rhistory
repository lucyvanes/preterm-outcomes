"z_brief_inhibit","z_brief_shift","z_brief_emo_control","z_brief_wm",
"z_brief_plan")
vars_reg2 <- c("z_wppsi_verb_comp","z_wppsi_visuospatial","z_wppsi_fluid_res","z_wppsi_wm","z_wppsi_proc_speed",
"z_wppsi_vocab","z_wppsi_nonverbal","z_wppsi_gen_abilities","z_wppsi_cog_prof")
for (v in vars_reg1){
lm1 <- lm(paste(v, "~ age4 + ses"), dat_prep)
dat_prep[v] <- lm1$resid
}
for (v in vars_reg2){
lm1 <- lm(paste(v, "~ ses"), dat_prep)
dat_prep[v] <- lm1$resid
}
dat_prep$age4 <- NULL
dat_prep$ses <- NULL
# permutation testing:
# for each permutation, shuffle rows in each column, re-compute PCA
#==============================================================================
# inspired by:
# http://bioops.info/2015/01/permutation-pca/
# the fuction to assess the significance of the principal components.
sign.pc<-function(x,R=5000,s=10, cor=T,...){
PC_nos
setwd("C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data")
dat <- read.csv("1_PCA_data.csv", header=T)
psych_vars <- c("z_ecbq_neg_affect","z_ecbq_surgency","z_ecbq_effort_control",
"z_emque_emo_contagion","z_emque_attent_others","z_emque_prosocial",
"z_sdq_emo","z_sdq_conduct","z_sdq_adhd","z_sdq_peer","z_sdq_prosocial",
"z_adhd_inattention","z_adhd_hyper",
"z_srs_soc_awareness","z_srs_soc_cog","z_srs_soc_comm","z_srs_soc_mot","z_srs_rrb")
cog_vars <- c("z_brief_inhibit","z_brief_shift","z_brief_emo_control","z_brief_wm",
"z_brief_plan",
"z_wppsi_verb_comp","z_wppsi_visuospatial","z_wppsi_fluid_res","z_wppsi_wm","z_wppsi_proc_speed",
"z_wppsi_vocab","z_wppsi_nonverbal","z_wppsi_gen_abilities","z_wppsi_cog_prof")
control_vars <- c("age4","ses")
all_vars <- c("id",psych_vars, cog_vars, control_vars)
setwd("C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data")
dat <- read.csv("1_PCA_data.csv", header=T)
psych_vars <- c("z_ecbq_neg_affect","z_ecbq_surgency","z_ecbq_effort_control",
"z_emque_emo_contagion","z_emque_attent_others","z_emque_prosocial",
"z_sdq_emo","z_sdq_conduct","z_sdq_adhd","z_sdq_peer","z_sdq_prosocial",
"z_adhd_inattention","z_adhd_hyper",
"z_srs_soc_awareness","z_srs_soc_cog","z_srs_soc_comm","z_srs_soc_mot","z_srs_rrb")
cog_vars <- c("z_brief_inhibit","z_brief_shift","z_brief_emo_control","z_brief_wm",
"z_brief_plan",
"z_wppsi_verb_comp","z_wppsi_visuospatial","z_wppsi_fluid_res","z_wppsi_wm","z_wppsi_proc_speed",
"z_wppsi_vocab","z_wppsi_nonverbal","z_wppsi_gen_abilities","z_wppsi_cog_prof")
control_vars <- c("age4","ses")
all_vars <- c("id",psych_vars, cog_vars, control_vars)
dat_prep <- dat[,all_vars]
dat_prep <- dat_prep[complete.cases(dat_prep),]
# correct certain variables for age at assessment and ses (IMD)
#=================================================================================
vars_reg1 <- c("z_ecbq_neg_affect","z_ecbq_surgency","z_ecbq_effort_control",
"z_emque_emo_contagion","z_emque_attent_others","z_emque_prosocial",
"z_sdq_emo","z_sdq_conduct","z_sdq_adhd","z_sdq_peer","z_sdq_prosocial",
"z_adhd_inattention","z_adhd_hyper",
"z_srs_soc_awareness","z_srs_soc_cog","z_srs_soc_comm","z_srs_soc_mot","z_srs_rrb",
"z_brief_inhibit","z_brief_shift","z_brief_emo_control","z_brief_wm",
"z_brief_plan")
vars_reg2 <- c("z_wppsi_verb_comp","z_wppsi_visuospatial","z_wppsi_fluid_res","z_wppsi_wm","z_wppsi_proc_speed",
"z_wppsi_vocab","z_wppsi_nonverbal","z_wppsi_gen_abilities","z_wppsi_cog_prof")
for (v in vars_reg1){
lm1 <- lm(paste(v, "~ age4 + ses"), dat_prep)
dat_prep[v] <- lm1$resid
}
for (v in vars_reg2){
lm1 <- lm(paste(v, "~ ses"), dat_prep)
dat_prep[v] <- lm1$resid
}
dat_prep$age4 <- NULL
dat_prep$ses <- NULL
#================
# Run PCA
#================
pcadat <- dat_prep
pcadat$id <- NULL
pcadat_scaled <- as.data.frame(scale(pcadat, center=T, scale=T))
# permutation testing:
# for each permutation, shuffle rows in each column, re-compute PCA
#==============================================================================
# taken from:
# http://bioops.info/2015/01/permutation-pca/
# the fuction to assess the significance of the principal components.
sign.pc<-function(x,R=5000,s=10, cor=T,...){
pc.out<-princomp(x,cor=cor,...)  # run PCA
pve=(pc.out$sdev^2/sum(pc.out$sdev^2))[1:s] # the proportion of variance of each PC
pve.perm<-matrix(NA,ncol=s,nrow=R)
for(i in 1:R){
x.perm<-apply(x,2,sample)
pc.perm.out<-princomp(x.perm,cor=cor,...)
pve.perm[i,]=(pc.perm.out$sdev^2/sum(pc.perm.out$sdev^2))[1:s]
}
pval<-apply(t(pve.perm)>pve,1,sum)/R # calculate p-values
return(list(pve=pve,pval=pval))
}
pca_sign <- sign.pc(pcadat_scaled,cor=T)
pca_sign
#===========================================
#            Original PCA
#===========================================
out <- princomp(pcadat_scaled)
summary(out)
PCs_orig <- as.data.frame(out$scores)
PC_nos <- which(pca_sign$pval < 0.05)
PC_nos
PCs_orig_sig <- PCs_orig[,PC_nos]
names(PCs_orig_sig) <- paste("orig_PC", PC_nos, sep="")
PC_nos
paste("half1_PC",PC_nos)
c("half1_PC1","half1_PC2","half1_PC3","half1_PC4",
"half2_PC1","half2_PC2","half2_PC3","half2_PC4")
c(paste("half1_PC",PC_nos),paste("half2_PC",PC_nos))
c(paste("half1_PC",PC_nos,sep=""),paste("half2_PC",PC_nos,sep=""))
#===========================================
#        Split-half anaylysis
#===========================================
n_half <- round(dim(pcadat_scaled)[1] * 0.5)
H <- 1000 # how many split half replications
loadings_cor <- data.frame(matrix(nrow=H, ncol=length(PC_nos)))
loadings_sig <- data.frame(matrix(nrow=H, ncol=length(PC_nos)*2))
names(loadings_sig) <- c(paste("half1_PC",PC_nos,sep=""),paste("half2_PC",PC_nos,sep=""))
for (n in names(loadings_sig)){
loadings_sig[n] <- 0
}
#===========================================
#        Split-half anaylysis
#===========================================
n_half <- round(dim(pcadat_scaled)[1] * 0.5)
H <- 1000 # how many split half replications
loadings_cor <- data.frame(matrix(nrow=H, ncol=length(PC_nos)))
loadings_sig <- data.frame(matrix(nrow=H, ncol=length(PC_nos)*2))
names(loadings_sig) <- c(paste("half1_PC",PC_nos,sep=""),paste("half2_PC",PC_nos,sep=""))
for (n in names(loadings_sig)){
loadings_sig[n] <- 0
}
for (h in 1:H){
print(paste("Split half ", h, sep=""), quote=F)
print("============================")
half1 <- sort(sample(1:dim(pcadat_scaled)[1], size=n_half))
half2 <- setdiff(1:dim(pcadat_scaled)[1], half1)
pcadat1 <- pcadat_scaled[half1,]
pcadat2 <- pcadat_scaled[half2,]
out1 <- princomp(pcadat1)
out2 <- princomp(pcadat2)
loadings1 <- out1$loadings[,PC_nos]
loadings2 <- out2$loadings[,PC_nos]
loadings_cor[h,] <-  diag(cor(loadings1, loadings2)) # take abs?
pca_sign1 <- sign.pc(pcadat1,cor=T)
pca_sign2 <- sign.pc(pcadat2,cor=T)
for (p in PC_nos){
if (pca_sign1$pval[p] < 0.05){
loadings_sig[h,p] <- 1
}
if (pca_sign2$pval[p] < 0.05){
loadings_sig[h,p+4] <- 1
}
}
loadings_sig[h,loadings_sig$haf1_PC1] <-
print(which(pca_sign1$pval < 0.05))
print(which(pca_sign2$pval < 0.05))
print(loadings_cor[h,])
}
#===========================================
#        Split-half anaylysis
#===========================================
n_half <- round(dim(pcadat_scaled)[1] * 0.5)
H <- 1000 # how many split half replications
loadings_cor <- data.frame(matrix(nrow=H, ncol=length(PC_nos)))
loadings_sig <- data.frame(matrix(nrow=H, ncol=length(PC_nos)*2))
names(loadings_sig) <- c(paste("half1_PC",PC_nos,sep=""),paste("half2_PC",PC_nos,sep=""))
for (n in names(loadings_sig)){
loadings_sig[n] <- 0
}
for (h in 1:H){
print(paste("Split half ", h, sep=""), quote=F)
print("============================")
half1 <- sort(sample(1:dim(pcadat_scaled)[1], size=n_half))
half2 <- setdiff(1:dim(pcadat_scaled)[1], half1)
pcadat1 <- pcadat_scaled[half1,]
pcadat2 <- pcadat_scaled[half2,]
out1 <- princomp(pcadat1)
out2 <- princomp(pcadat2)
loadings1 <- out1$loadings[,PC_nos]
loadings2 <- out2$loadings[,PC_nos]
loadings_cor[h,] <-  diag(cor(loadings1, loadings2)) # take abs?
pca_sign1 <- sign.pc(pcadat1,cor=T)
pca_sign2 <- sign.pc(pcadat2,cor=T)
for (p in PC_nos){
if (pca_sign1$pval[p] < 0.05){
loadings_sig[h,p] <- 1
}
if (pca_sign2$pval[p] < 0.05){
loadings_sig[h,p+4] <- 1
}
}
# loadings_sig[h,loadings_sig$haf1_PC1] <-
print(which(pca_sign1$pval < 0.05))
print(which(pca_sign2$pval < 0.05))
print(loadings_cor[h,])
}
colMeans(loadings_cor, na.rm=T)
mean(loadings_sig$half1_PC1)
mean(loadings_sig$half2_PC1)
# % both halves significant PC
dim(loadings_sig[loadings_sig$half1_PC1==1 & loadings_sig$half2_PC1==1,])[1] / 1000
pca_sign1 <- sign.pc(pcadat1,cor=T)
# % both halves significant PC
dim(loadings_sig[loadings_sig$half1_PC1==1 & loadings_sig$half2_PC1==1,])[1] / H
mean(loadings_sig$half1_PC1)
mean(loadings_sig$half2_PC1)
# Get some stats
#==============================
colMeans(loadings_cor, na.rm=T)
loadings_sig
str(loadings_sig)
#===============================================================================
# inspect loadings
#===============================================================================
loadings <- out$loadings
loadings <- as.data.frame.matrix(loadings)
# http://strata.uga.edu/8370/lecturenotes/principalComponents.html
threshold <- sqrt(1/ncol(pcadat))  # cutoff for 'important' loadings
loadings[abs(loadings) < threshold] <- NA
loadings
dat_prep$PC1_all <- out$scores[,1]
dat_prep$PC2_all <- out$scores[,2]
dat_prep$PC3_all <- out$scores[,3]
dat$id <- factor(dat$id)
dat_prep$id <- factor(dat_prep$id)
dat$PC1_all <- NA
dat$PC2_all <- NA
dat$PC3_all <- NA
for (i in levels(dat_prep$id)){
dat$PC1_all[dat$id==i] <- dat_prep$PC1_all[dat_prep$id==i]
dat$PC2_all[dat$id==i] <- dat_prep$PC2_all[dat_prep$id==i]
dat$PC3_all[dat$id==i] <- dat_prep$PC3_all[dat_prep$id==i]
}
View(da)
View(dat)
#=============================================================
# Plot heatmaps
#=============================================================
library(corrplot)
library(RColorBrewer)
library(dplyr)
# prepare correlations
#========================
pcs_indx <- which(!is.na(match(names(dat_prep), c("PC1_all","PC2_all","PC3_all"))))
vars_indx <- which(!is.na(match(names(dat_prep), c(psych_vars, cog_vars))))
n_p <- length(pcs_indx)
n_v <- length(vars_indx)
cor_data <- dat_prep[,c(pcs_indx,vars_indx)]
M <- cor(cor_data)
M <- M[(n_p+1):(n_v+n_p), 1:n_p]
dimnames(M)[[1]] <- c("Negative affect", "Surgency","Effortful control", # ECBQ
"Emotion contagion","Attention to others' feelings","Prosocial actions", # EmQue
"Emotional symptoms","Conduct problems","Hyperactivity-inattention","Peer problems", # SDQ
"Prosocial behaviour", # SDQ
"Inattention","Hyperactivity", # ADHD
"Social awareness","Social cognition","Social communication",
"Social motivation","Repetitive behaviours", # SRS
"Inhibit","Shift","Emotional control", "Working memory","Planning/Organisation", # BRIEF
"Verbal comprehension","Visuospatial skills",
"Fluid reasoning","Working memory","Processing speed","Vocabulary",
"Nonverbal","General abilities","Cognitive proficiency") # WPPSI
dimnames(M)[[2]] <- c("PC1","PC2","PC3")
res1 <- cor.mtest(cor_data, conf.level = .95)
res1$p <- res1$p[(n_p+1):(n_v+n_p), 1:n_p]
res1$lowCI <- res1$lowCI[(n_p+1):(n_v+n_p), 1:n_p]
res1$uppCI <- res1$uppCI[(n_p+1):(n_v+n_p), 1:n_p]
# prepare loadings
#====================
loadings <- as.matrix(loadings[,1:3])
dimnames(loadings)[[1]] <- c("Negative affect", "Surgency","Effortful control", # ECBQ
"Emotion contagion","Attention to others' feelings","Prosocial actions", # EmQue
"Emotional symptoms","Conduct problems","Hyperactivity-inattention","Peer problems", # SDQ
"Prosocial behaviour", # SDQ
"Inattention","Hyperactivity", # ADHD
"Social awareness","Social cognition","Social communication",
"Social motivation","Repetitive behaviours", # SRS
"Inhibit","Shift","Emotional control", "Working memory","Planning/Organisation", # BRIEF
"Verbal comprehension","Visuospatial skills",
"Fluid reasoning","Working memory","Processing speed","Vocabulary",
"Nonverbal","General abilities","Cognitive proficiency") # WPPSI
dimnames(loadings)[[2]] <- c("PC1","PC2","PC3")
# plots
#===================================================
col=rev(colorRampPalette(brewer.pal(n=11, name="RdBu"))(100))
# jpeg("corrplot_R.jpeg")
corrplot(M, p.mat = res1$p,method="color", col=col,
tl.col="black",tl.cex=1, # tl.offset=0.5,
cl.align="l",cl.ratio = 1,
insig = "blank",
addgrid.col="grey")
# plots
#===================================================
col <- rev(colorRampPalette(brewer.pal(n=11, name="RdBu"))(100))
corrplot(M, p.mat = res1$p,method="color", col=col,
tl.col="black",tl.cex=1, # tl.offset=0.5,
cl.align="l",cl.ratio = 1,
insig = "blank",
addgrid.col="grey")
corrplot(loadings, is.corr=F, method="color", col=col,
tl.col="black",tl.cex=1,
cl.align="l", cl.ratio = 1,
na.label="square",na.label.col="white",
addgrid.col="grey")
#=====================================
#           prepare data
#=====================================
mask_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/masks"
data_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/preprocessed"
output_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/output"
permuted_output_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/permuted_output"
setwd(mask_dir)
mask <- readnii("AAL_atlas_mask_2mm.nii.gz")       # name of mask (can be whole-brain mask)
library(NMF)
library(NNLM)
library(pracma)
library(dplyr)
library(neurobase)
library(parallel)
#=====================================
#           prepare data
#=====================================
mask_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/masks"
data_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/preprocessed"
output_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/output"
permuted_output_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/permuted_output"
setwd(mask_dir)
mask <- readnii("AAL_atlas_mask_2mm.nii.gz")       # name of mask (can be whole-brain mask)
mask_vec <- c(mask)
outside_mask <- which(mask_vec==0)
inside_mask <- which(mask_vec==1)
length(mask[mask==1])
setwd(data_dir)
scans <- dir(pattern="smoothedJacs")              # some pattern to find your images
example_scan <- readnii(scans[1])
example_scan_vec <- c(example_scan)
scandata <- as.data.frame(matrix(nrow=length(inside_mask), ncol=length(scan)))
for (s in 1:length(scans)){
print(scans[s])
t <- readnii(scans[s])
tvec <- c(t)
tvec_masked <- tvec[mask==1]
scandata[,s] <- tvec_masked
}
dat <- scandata
dat <- exp(dat) # exponentiate jacobians
dat <- as.matrix(dat)
# random permutation of matrix
dat_permut <- dat
set.seed(12345)
for (c in 1:ncol(dat)){
dat_permut[,c] <- dat[sample(nrow(dat)),c]
}
ranks <- seq(2,20,1) # choose ranks to run over
numCores <- 9 # choose number of cores for parallelising; or use detectCores()
numCores <- 1
setwd(output_dir)
rank_res <- mclapply(ranks,nnmf_test_ranks, data=dat,  WH_true=1, mc.cores=numCores)
# source nnmf_test_ranks function
source("C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/2_NNMF/nnmf_test_ranks.R")
rank_res <- mclapply(ranks,nnmf_test_ranks, data=dat,  WH_true=1, mc.cores=numCores)
ranks <- seq(2,4,1) # choose ranks to run over
# source nnmf_test_ranks function
source("C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/2_NNMF/nnmf_test_ranks.R")
rank_res <- mclapply(ranks,nnmf_test_ranks, data=dat,  WH_true=1, mc.cores=numCores)
library(NMF)
library(pracma)
library(dplyr)
library(neurobase)
mask_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/masks"
data_dir <- "C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data/NNMF_data/preprocessed"
setwd(mask_dir)
mask <- readnii("AAL_atlas_mask_2mm.nii.gz") # read mask into R
mask_vec <- c(mask)                          # vectorise mask, this is now a vector of 0s and 1s
outside_mask <- which(mask_vec==0)           # define what is outside and inside mask
inside_mask <- which(mask_vec==1)
length(mask[mask==1])
setwd(data_dir)
scans <- dir(pattern="smoothedJacs_2mm_AAL_masked")   # some pattern to find your images
example_scan <- readnii(scans[1])
example_scan_vec <- c(example_scan)
scandata <- as.data.frame(matrix(nrow=length(inside_mask), ncol=length(scan)))
for (s in 1:length(scans)){
print(scans[s])
t <- readnii(scans[s])
tvec <- c(t)
tvec_masked <- tvec[mask==1]
scandata[,s] <- tvec_masked
}
dat <- scandata
dat <- exp(dat) # exponentiate jacobians to get positive values
dat <- as.matrix(dat)
dim(dat)     # this should be voxels X subjects
best_rank <- 15   # choose rank (ascertained with previous script)
# rerun NNMF with chosen number of ranks, using nndsdv
res_nmf <- nmf(dat, best_rank, method="lee",seed="nndsvd", .options="v")
setwd("C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data")
dat <- read.csv("3_Analysis_data.csv", header=T)
dat$id <- factor(dat$id)
dim(dat)
anova(lm(cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + cog_stim, dat))
summary(lm(cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + cog_stim, dat))
# 3.2.	Association between stimulating home environment and outcomes
#=======================================================================
setwd("C:/Users/vanes/OneDrive/Documents/GitHub/preterm-outcomes/data")
dat <- read.csv("3_Analysis_data.csv", header=T)
dat$id <- factor(dat$id)
dim(dat)
# omnibus test
anova(lm(cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + cog_stim, dat))
# univariate tests
summary(lm(cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + cog_stim, dat))
# plot effect
#=============
fit <- lm(PC1_all ~ gest_weeks + sex + total_homes, dat)
# plot effect
#=============
fit <- lm(PC1_all ~ gest_weeks + sex + cog_stim, dat)
effect_plot(fit, pred = cog_stim, interval = TRUE, plot.points = TRUE, point.size=1.5,
point.alpha=0.4, point.color="black",
x.label="Cognitively stimulating parenting", y.label="PC1 partial residuals",
partial.residuals = TRUE)
library(jtools)
effect_plot(fit, pred = cog_stim, interval = TRUE, plot.points = TRUE, point.size=1.5,
point.alpha=0.4, point.color="black",
x.label="Cognitively stimulating parenting", y.label="PC1 partial residuals",
partial.residuals = TRUE)
jac <- read.csv("rank15_weighted_means.csv", header=T)
getwd()
jac <- read.csv("rank15_weighted_means.csv", header=T)
jac$id <- factor(jac$id)
dim(jac)
dat <- merge(dat, jac, by="id")
dim(dat)
# use multivariate multiple regression as omnibus test to protect from type I error
for (n in NNMF_networks){
print("#=============================#")
print(n)
print("#=============================#")
m <- lm(as.formula(paste("cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + pma + ",n)), dat)
m1 <- lm(as.formula(paste("PC1_all ~ gest_weeks + sex + pma + ",n)), dat)
m2 <- lm(as.formula(paste("PC2_all ~ gest_weeks + sex + pma + ",n)), dat)
m3 <- lm(as.formula(paste("PC3_all ~ gest_weeks + sex + pma + ",n)), dat)
print(anova(m))
# print(summary(m3))
}
NNMF_networks <- paste("mean_NNMF",1:15, sep="")
# use multivariate multiple regression as omnibus test to protect from type I error
for (n in NNMF_networks){
print("#=============================#")
print(n)
print("#=============================#")
m <- lm(as.formula(paste("cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + pma + ",n)), dat)
m1 <- lm(as.formula(paste("PC1_all ~ gest_weeks + sex + pma + ",n)), dat)
m2 <- lm(as.formula(paste("PC2_all ~ gest_weeks + sex + pma + ",n)), dat)
m3 <- lm(as.formula(paste("PC3_all ~ gest_weeks + sex + pma + ",n)), dat)
print(anova(m))
# print(summary(m3))
}
# use multivariate multiple regression as omnibus test to protect from type I error
for (n in NNMF_networks){
print("#=============================#")
print(n)
print("#=============================#")
m <- lm(as.formula(paste("cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + pma + ",n)), dat)
print(anova(m))
}
# Plot effects
#==============
fit12_2 <- lm(PC2_all ~ gest_weeks + sex + pma + mean_NNMF12, dat)
effect_plot(fit12_2, pred = mean_NNMF12, interval = TRUE, plot.points = TRUE,
point.size=1.5,
point.alpha=0.4, point.color="black",
x.label="NNMF12 volume", y.label="PC2 partial residuals", partial.residuals = TRUE)
anova(lm(cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + pma +
cog_stim + mean_NNMF12, dat))
# test whether adding interaction effect improves models
lm1 <- lm(PC1_all ~ gest_weeks + sex + pma + total_homes, dat)
lm2 <- lm(PC1_all ~ gest_weeks + sex + pma + total_homes + mean_NNMF12, dat)
# test whether adding interaction effect improves models
lm1 <- lm(PC1_all ~ gest_weeks + sex + pma + cog_stim, dat)
lm2 <- lm(PC1_all ~ gest_weeks + sex + pma + cog_stim + mean_NNMF12, dat)
anova(lm1, lm2)
# test whether adding interaction effect improves models
lm1 <- lm(PC1_all ~ gest_weeks + sex + pma + cog_stim + mean_NNMF12, dat)
lm2 <- lm(PC1_all ~ gest_weeks + sex + pma + cog_stim * mean_NNMF12, dat)
anova(lm1, lm2)
summary(lm1)
summary(lm2)
lm1 <- lm(PC2_all ~ gest_weeks + sex + pma + cog_stim + mean_NNMF12, dat)
lm2 <- lm(PC2_all ~ gest_weeks + sex + pma + cog_stim * mean_NNMF12, dat)
anova(lm1, lm2)
summary(lm1)
summary(lm2)
lm1 <- lm(PC3_all ~ gest_weeks + sex + pma + cog_stim + mean_NNMF12, dat)
lm2 <- lm(PC3_all ~ gest_weeks + sex + pma + cog_stim * mean_NNMF12, dat)
anova(lm1, lm2)
summary(lm1)
summary(lm2)
an1 <- lm(cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + pma + cog_stim + mean_NNMF12, dat)
an2 <- lm(cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + pma + cog_stim * mean_NNMF12, dat)
summary(an1)
an1 <- anova(lm(cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + pma + cog_stim + mean_NNMF12, dat))
an2 <- anova(lm(cbind(PC1_all, PC2_all, PC3_all) ~ gest_weeks + sex + pma + cog_stim * mean_NNMF12, dat))
summary(an1)
an1
dat$id
as.numeric(as.character(dat$id))
as.numeric(as.character(dat$id)) * 2
